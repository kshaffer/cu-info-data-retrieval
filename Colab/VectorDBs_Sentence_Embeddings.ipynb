{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings with Sentence Transformers",
   "id": "f321c7012472706d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Install any packages you don't already have.",
   "id": "9988b08ca7fe166f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install chromadb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import Client, Settings\n",
    "from sentence_transformers.util import cos_sim\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from pymongo import MongoClient"
   ],
   "id": "f87a51d29839a9bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import a semantic sentence model, against which you will calculate embeddings. There are MANY to choose from. We'll start with a lightweight, all-purpose model suggested by our reading, `all-MiniLM-L6-v2`. I've also preloaded code here for a much more robust multilingual model that I like to use for narrative discovery and translation tasks, `LaBSE`. But there are many more out there...",
   "id": "4250fed9b1b70c39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# a good, lightweight model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# a better but much larger model, LaBSE\n",
    "# model = SentenceTransformer('sentence-transformers/LaBSE')"
   ],
   "id": "4700098a2d912815",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's read in some documents and generate embeddings for them that align with the chosen model.",
   "id": "741d825305ec92af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read in some documents from facts.txt\n",
    "with open('facts.txt', 'r') as f:\n",
    "    documents = f.readlines()\n",
    "    documents = [x.strip() for x in documents]"
   ],
   "id": "27003ee4b59113f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "documents[:10]",
   "id": "f040926f082300ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embeddings = model.encode(documents)",
   "id": "32a72cc0e5ec633d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print the number of dimensions in these embeddings\n",
    "print(len(embeddings[0]))\n",
    "\n",
    "# print the embedding of the first sentence\n",
    "embeddings[0]"
   ],
   "id": "28b57889036cae16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# show all of the embeddings\n",
    "embeddings"
   ],
   "id": "6dacca26fa5483c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll integrate with MongoDB later, but for now, let's use a simple, in-memory vector database, ChromaDB.",
   "id": "7fac3eca624f2778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# initialize a ChromaDB client and collection\n",
    "chroma_client = Client(Settings(is_persistent = False))\n",
    "collection = chroma_client.create_collection(name = 'docs')"
   ],
   "id": "d74c6d61f3f440ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store documents and embeddings in ChromaDB\n",
    "collection.add(embeddings = [e.tolist() for e in embeddings],\n",
    "               documents = documents,\n",
    "               ids = [f\"doc_{i}\" for i in range(len(documents))])"
   ],
   "id": "20dea292ade57538",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's ask a question of this database.",
   "id": "6261f2d3d1b8f11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = 'What is the capital of France?'\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "print(query_embedding)"
   ],
   "id": "17e5692b98a81b48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# search for similar documents\n",
    "# (ChromaDB uses cosine similarity by default)\n",
    "results = collection.query(query_embeddings = [query_embedding.tolist()],\n",
    "                           n_results = 1)\n",
    "\n",
    "for doc in results['documents'][0]:\n",
    "    print(doc)"
   ],
   "id": "f9942693899f58d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Try again, but ask it to return more results.",
   "id": "c268acc2df0c997c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# search for similar documents\n",
    "results = collection.query(query_embeddings = [query_embedding.tolist()],\n",
    "                           n_results = 5)\n",
    "\n",
    "for doc in results['documents'][0]:\n",
    "    print(doc)"
   ],
   "id": "655c835dc7ffa252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's peer under the hood a bit and see what's happening...",
   "id": "b4bf866b55dd7ff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# cosine similarity of the question's embedding and the top answer's embedding\n",
    "cos_sim(query_embedding,\n",
    "        model.encode(results['documents'][0][0]))"
   ],
   "id": "3eedb642d701cb56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make a dataframe of the top five results\n",
    "results_list = []\n",
    "\n",
    "for doc in results['documents'][0]:\n",
    "    results_list.append([query, doc, float(cos_sim(query_embedding,\n",
    "                                                   model.encode(doc)))])\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df"
   ],
   "id": "937f02648a76ab71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# try another question\n",
    "query = 'Who is the president of the United States?'\n",
    "query_embedding = model.encode(query)\n",
    "results = collection.query(query_embeddings = [query_embedding.tolist()],\n",
    "                           n_results = 1)\n",
    "\n",
    "for doc in results['documents'][0]:\n",
    "    print(doc)"
   ],
   "id": "f3c3a04bb3748dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MongoDB Vector Functionality",
   "id": "11b062a8d6593503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's use MongoDB's native vector search capabilities. As of September 2025, MongoDB Community Edition supports vector search locally!",
   "id": "5d261911a4e6858c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, we need to [install a couple of things](https://www.mongodb.com/docs/atlas/cli/current/atlas-cli-deploy-local/).\n",
    "\n",
    "- AtlasCLI (This may make you setup an account on their Atlas platform and/or prompt you to setup a database in their cloud service, but you don't have to! Once you have this installed, you can create the database locally.)\n",
    "- Docker (This needs to be on and running in the background, but otherwise you don't have to touch it.)\n",
    "\n",
    "Once both are installed, run `atlas setup` followed by `atlas deployments setup` to get going.\n",
    "\n",
    "NOTE: This will be a separate MongoDB instance on your machine from what we've already been working with,and it will give you a different port number to connect to."
   ],
   "id": "8043675a6c5d4ab8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Connect to MongoDB (local ATLAS installation)\n",
    "client = MongoClient('mongodb://localhost:55784/?directConnection=true&serverSelectionTimeoutMS=2000&appName=mongosh+2.5.9') # use the connection info Atlas gives you.\n",
    "db = client['vector_search_demo']\n",
    "collection = db['documents']\n",
    "\n",
    "# Clear any existing documents\n",
    "# collection.delete_many({})\n",
    "print(\"Connected to MongoDB\")"
   ],
   "id": "a466da41466a9332",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store documents and embeddings in MongoDB\n",
    "docs_to_insert = []\n",
    "for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "    docs_to_insert.append({\n",
    "        'text': doc,\n",
    "        'embedding': embedding.tolist()\n",
    "    })\n",
    "\n",
    "result = collection.insert_many(docs_to_insert)\n",
    "print(f\"Inserted {len(result.inserted_ids)} documents into MongoDB\")"
   ],
   "id": "f22ff43842e76d3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "collection.find_one()",
   "id": "397031115eef7312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a vector search index\n",
    "# This enables efficient vector similarity search in MongoDB\n",
    "try:\n",
    "    collection.create_search_index(\n",
    "        model={\n",
    "            \"definition\": {\n",
    "                \"mappings\": {\n",
    "                    \"dynamic\": True,\n",
    "                    \"fields\": {\n",
    "                        \"embedding\": {\n",
    "                            \"type\": \"vector\",\n",
    "                            \"numDimensions\": 384,\n",
    "                            \"similarity\": \"cosine\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"name\": \"vector_index\"\n",
    "        }\n",
    "    )\n",
    "    print(\"Vector search index created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Index creation note: {e}\")\n",
    "    print(\"Index may already exist or require MongoDB 7.0+ with vector search enabled\")"
   ],
   "id": "cb39d89c2b120f4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's ask a question of this database.",
   "id": "7b754113c506129b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = 'What is the capital of France?'\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "print(query_embedding)"
   ],
   "id": "d7c7df636ba8b576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform vector search using MongoDB's $vectorSearch aggregation stage\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding.tolist(),\n",
    "            \"numCandidates\": 100,\n",
    "            \"limit\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"text\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = list(collection.aggregate(pipeline))\n",
    "\n",
    "for result in results:\n",
    "    print(result['text'])"
   ],
   "id": "63500183a15c10f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Try again, but ask it to return more results.",
   "id": "c0b80d9089ae4de7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Search for top 5 similar documents\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding.tolist(),\n",
    "            \"numCandidates\": 100,\n",
    "            \"limit\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"text\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = list(collection.aggregate(pipeline))\n",
    "\n",
    "for result in results:\n",
    "    print(result['text'])"
   ],
   "id": "978c5bdf2c530dc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's peer under the hood a bit and see what's happening...",
   "id": "5413e4f4679349da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Try another question\n",
    "query = 'Who is the president of the United States?'\n",
    "# query = 'Does the earth orbit around Mars?'\n",
    "# query = 'How many sides does a pentagon have?'\n",
    "# query = 'How many sides does a nonagon have?'\n",
    "# query = 'What is hockey?'\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding.tolist(),\n",
    "            \"numCandidates\": 100,\n",
    "            \"limit\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"text\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = list(collection.aggregate(pipeline))\n",
    "\n",
    "for result in results:\n",
    "    print(result['text'])"
   ],
   "id": "faff440ecf6cd422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3261d07d0f308349",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
